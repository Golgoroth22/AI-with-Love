# День 17. Первый RAG-запрос

1. Реализуйте функцию: «вопрос → поиск релевантных чанков → объединение с вопросом → запрос к LLM»
2. Сравните ответ модели с RAG и без RAG
3. Сделайте вывод: где RAG помог, где — нет

- `Результат`: Агент с двумя режимами (с RAG / без RAG) и сравнением
- `Формат`: Видео + Код

[Видео результата](https://drive.google.com/file/d/1cZMopGCgL3y6MXQ4k7W_zi0YmXFu-k-q/view?usp=sharing)

Код mcp-сервера в директории **server/**

1. Экран с аи-ассистентом с возможностью использования mcp-сервера.
   У mcp-сервера 4 тулзы:
- get_joke - Получить случайную шутку из JokeAPI.
- save_joke - Сохранить шутку в локальную датабазу на сервере.
- get_saved_jokes - Достать все шутки из локальной датабазы на сервере.
- run_tests - Запуск тестов MCP сервера в изолированном Docker контейнере. Выполняет все модульные тесты и возвращает результаты.
- semantic_search - Поиск релевантных чанков из индексированных документов, используя семантическое сходство. 
2. Экран для работы с ollama на mcp-сервере. Можно как закинуть текст руками, так и файлы целиком. Чанки лежат в базе данных на сервере.

Вывод:
1. Без RAG модель даёт общий, шаблонный ответ на основе своих исходных знаний. В узкоспециализированных вопросах часто отвечает: «Эта информация мне недоступна» или приводит устаревшие/общие данные.
2. С RAG модель извлекает из базы точные фрагменты документов: цитирует актуальные спецификации, нормы, процедуры или детали. Ответ становится конкретным, с отсылками к источникам. Однако если в базе нет нужных данных, RAG не помогает.