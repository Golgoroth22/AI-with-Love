# День 19. Цитаты и источники

1. Доработайте свой RAG так, чтобы модель обязательно возвращала цитаты/ссылки на источники из базы
2. Протестируйте на 5 вопросах: есть ли ссылки в каждом ответе
3. Оцените, стало ли меньше «галлюцинаций»

- `Результат`: Ответы модели с обязательными ссылками на источники
- `Формат`: Видео + Код

[Видео результата](https://drive.google.com/file/d/1llNV41eMqu9fnu6xDH93YSyYOhXa65cA/view?usp=sharing)

Код mcp-сервера в директории **server/**

1. Экран с аи-ассистентом с возможностью использования mcp-сервера. У mcp-сервера 5 tools:
- get_joke - Получить случайную шутку из JokeAPI.
- save_joke - Сохранить шутку в локальную датабазу на сервере.
- get_saved_jokes - Достать все шутки из локальной датабазы на сервере.
- run_tests - Запуск тестов MCP сервера в изолированном Docker контейнере. Выполняет все модульные тесты и возвращает результаты.
- semantic_search - Поиск релевантных чанков из индексированных документов, используя семантическое сходство. 
2. Экран для работы с ollama на mcp-сервере. Можно как закинуть текст руками, так и файлы целиком. Чанки лежат в базе данных на сервере. Трещхолд фильтр реализован мануальный на стороне сервера @http_mcp_server.py

Вывод:
Фильтрация и реранкинг снизили галлюцинации, оставляя в контексте только релевантные данные с высокой степенью уверенности. Это особенно критично для фактологических и технических вопросов, где точность важнее полноты.
